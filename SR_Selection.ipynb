{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of SR correlation from cross-validation datat\n",
    "\n",
    "This workbook ranks and selects the expressions discovered for the bulk modulus correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import workflows as wf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as skmetrics \n",
    "from itertools import permutations\n",
    "import ast\n",
    "from collections import Counter\n",
    "import runSR as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def eqn_token(formula):\n",
    "    from io import StringIO\n",
    "    import tokenize\n",
    "\n",
    "    output = [token[1] for token in tokenize.generate_tokens(StringIO(formula).readline) if token[1]]\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# RETURN SORTED LIST OF TERMS ONLY, REMOVING THE COEFFICIENTS\n",
    "def list_terms(eqn1):\n",
    "    eqn1_token = eqn_token(eqn1)\n",
    "    lenlist = [len(e) for e in eqn_token(eqn1)] #Number of digits/letters in each term\n",
    "    coeff_inds = np.where([i > 10 for i in lenlist])[0].tolist() #Indices of terms with 10+ (coeff digits)\n",
    "    coeffs = eqn1_token[coeff_inds]\n",
    "\n",
    "    for coeff in coeffs:\n",
    "        eqn1 = eqn1.replace(coeff,'K')\n",
    "        \n",
    "    if eqn1[0]== '-': #get rid of first negative term\n",
    "        eqn1 = eqn1[1:]\n",
    "        \n",
    "    eqn1 = eqn1.replace(' - K','+K')\n",
    "    eqn1 = eqn1.replace(' + K','+K')\n",
    "    \n",
    "    eqn1 = eqn1.split(\"+K*\")\n",
    "    eqn1 = [eqni.split(\"+K\") for eqni in eqn1]\n",
    "    eqn1 = flatten(eqn1)\n",
    "    eqn1 = [eqni.split(\"K\") for eqni in eqn1]\n",
    "    eqn1 = flatten(eqn1)\n",
    "    try:\n",
    "        while True:\n",
    "            eqn1.remove('')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    for i,eqni in enumerate(eqn1):\n",
    "        if eqni[0] == '*':\n",
    "            eqn1[0] = eqni[1:]\n",
    "    eqn1 = sorted(eqn1)\n",
    "    \n",
    "    return eqn1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SR KFOLD RESULTS\n",
    "#=========================================\n",
    "file = 'SR_Kfold_BulkModulus_Results.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# ONLY INLCUDE TERMS WITH ALL [T,r,m]\n",
    "#=========================================\n",
    "df = df[df['eqn'].str.contains('m') & df['eqn'].str.contains('T') & df['eqn'].str.contains('r')]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# ADD N_Terms TO DF\n",
    "#=========================================\n",
    "N_terms = []\n",
    "for eqn in df.eqn:\n",
    "    lenlist = [len(e) for e in eqn_token(eqn)]\n",
    "    N = sum(i > 10 for i in lenlist) #THRESHOLD FOR EACH COEFFICIENT IS 10 SIGFIGS\n",
    "    N_terms.append(N)\n",
    "df['Nterms'] = N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goodness of fit for Training Set\n",
    "#=========================================\n",
    "\n",
    "viridis = cm.get_cmap('viridis',3)\n",
    "colors = viridis(range(5))\n",
    "shapes = [\"s\",\"^\",\"o\"]\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "for i in range(len(df)):\n",
    "    feat = df.feats[i]\n",
    "    n_exp = len(ast.literal_eval(df.exp[i]))\n",
    "    n_terms = df.Nterms[i] \n",
    "    y2 = df.rmse_train[i]\n",
    "    y = df.r2_train[i]\n",
    "    ax[0].scatter(n_terms, y, s=100, marker=shapes[feat-2],facecolors='none',edgecolors=colors[feat-1],linewidths=2,label=feat)\n",
    "    ax[1].scatter(n_terms, y2*1000, s=100, marker=shapes[feat-2],facecolors='none',edgecolors=colors[feat-1],linewidths=2,label=feat)\n",
    "ax[0].set_ylabel('Training R$^2$', fontsize=12)\n",
    "ax[0].set_xlabel('Number of terms in discovered expression', fontsize=12)\n",
    "ax[0].set_ylim([0.99,1])\n",
    "\n",
    "ax[1].set_ylabel('Training RMSE (MPa)', fontsize=12)\n",
    "ax[1].set_xlabel('Number of terms in discovered expression', fontsize=12)\n",
    "ax[1].set_ylim([10,70])\n",
    "plt.show()\n",
    "# fig.savefig('training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goodness of fit for Test Set\n",
    "#=========================================\n",
    "\n",
    "viridis = cm.get_cmap('viridis',3)\n",
    "colors = viridis(range(5))\n",
    "shapes = [\"s\",\"^\",\"o\"]\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "for i in range(len(df)):\n",
    "    feat = df.feats[i]\n",
    "    n_exp = len(ast.literal_eval(df.exp[i]))\n",
    "    n_terms = df.Nterms[i] \n",
    "    y2 = df.rmse_test[i]\n",
    "    y = df.r2_test[i]\n",
    "    ax[0].scatter(n_terms, y, s=100, marker=shapes[feat-2],facecolors='none',edgecolors=colors[feat-1],linewidths=2,label=feat)\n",
    "    ax[1].scatter(n_terms, y2*1000, s=100, marker=shapes[feat-2],facecolors='none',edgecolors=colors[feat-1],linewidths=2,label=feat)\n",
    "ax[0].set_ylabel('Training R$^2$', fontsize=12)\n",
    "ax[0].set_xlabel('Number of terms in discovered expression', fontsize=12)\n",
    "ax[0].set_ylim([0.99,1])\n",
    "\n",
    "ax[1].set_ylabel('Training RMSE (MPa)', fontsize=12)\n",
    "ax[1].set_xlabel('Number of terms in discovered expression', fontsize=12)\n",
    "ax[1].set_ylim([10,70])\n",
    "plt.show()\n",
    "# fig.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUATION EXPRESSION TERM MATCHING\n",
    "#=========================================\n",
    "term_str = []\n",
    "term_list = []\n",
    "for eqn in df.eqn:\n",
    "    tlist = list_terms(eqn)\n",
    "    tstr = ' , '.join(tlist)\n",
    "    term_str.append(tstr)\n",
    "    term_list.append(tlist)\n",
    "\n",
    "df['term_str'] = term_str\n",
    "df['term_list'] = term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE RANKING TABLE OF TERMS\n",
    "#=========================================\n",
    "letter_counts = Counter(df['term_str'])\n",
    "df_terms = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df_terms = df_terms.sort_values(by=[0],ascending=False)\n",
    "df_terms = df_terms.rename(columns={0: 'occur'})\n",
    "\n",
    "term_strs = df_terms.index.tolist()\n",
    "r2_test_array = []\n",
    "r2_train_array = []\n",
    "rmse_test_array = []\n",
    "rmse_train_array = []\n",
    "Nterms = []\n",
    "\n",
    "for term_str in term_strs:\n",
    "#     print(term_str)\n",
    "    df_res = df[(df['term_str']==term_str)]\n",
    "    r2_test_array.append(np.max(df_res.r2_test))\n",
    "    rmse_test_array.append(np.min(df_res.rmse_test))\n",
    "    r2_train_array.append(np.max(df_res.r2_train))\n",
    "    rmse_train_array.append(np.min(df_res.rmse_train))\n",
    "    Nterms.append(np.mean(df_res.Nterms))\n",
    "    \n",
    "df_terms['Nterms'] = Nterms\n",
    "df_terms['r2_test'] = r2_test_array\n",
    "df_terms['r2_train'] = r2_train_array\n",
    "df_terms['rmse_test'] = rmse_test_array\n",
    "df_terms['rmse_train'] = rmse_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW RANKING IN TERMS OF TEST RMSE\n",
    "#=========================================\n",
    "result = df_terms.sort_values(by=['rmse_test'],ascending=True).head(10)\n",
    "display(result)\n",
    "# print(result.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW RANKING OF 6 TERM EXPRESSIONS BASED ON OCCURANCE\n",
    "#=========================================\n",
    "result = df_terms[(df_terms.Nterms==6)].sort_values(by=['occur'],ascending=False).head(10)\n",
    "display(result)\n",
    "# print(result.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above ranking, the second row expression has the best goodness-of-fit and also minimizes the RMSE while maintaining occurances among a high proportion of cross-validation runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
